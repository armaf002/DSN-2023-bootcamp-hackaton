{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data \n",
    "train = pd.read_csv('/workspaces/codespaces-jupyter/Housing_dataset_train.csv')\n",
    "test = pd.read_csv('/workspaces/codespaces-jupyter/Housing_dataset_test.csv')\n",
    "sample_sub = pd.read_csv('/workspaces/codespaces-jupyter/Sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna(train.mean(), inplace=True)\n",
    "train.fillna(train.mode().iloc[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train.copy()\n",
    "test2 = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_feature_engineering(df):\n",
    "    # Handle Categorical Features: One-hot encoding for 'loc' and 'title'\n",
    "    ohe = OneHotEncoder(drop='first', sparse=False)\n",
    "    encoded_features = ohe.fit_transform(df[['loc', 'title']])\n",
    "    encoded_df = pd.DataFrame(encoded_features, columns=ohe.get_feature_names_out(['loc', 'title']))\n",
    "\n",
    "    # Concatenate the encoded features with the original DataFrame\n",
    "    df = pd.concat([df.drop(['loc', 'title'], axis=1), encoded_df], axis=1)\n",
    "\n",
    "    # Creating Interaction Features\n",
    "    df['total_rooms'] = df['bedroom'] + df['bathroom']\n",
    "    df['bedroom_to_bathroom_ratio'] = df['bedroom'] / df['bathroom']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Perform feature engineering using the function\n",
    "train2 = perform_feature_engineering(train2)\n",
    "test2 = perform_feature_engineering(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "X = train2.drop(['price'], axis=1)\n",
    "y = train2['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model:\n",
      "               Model           RMSE\n",
      "2  Gradient Boosting  624866.595563\n"
     ]
    }
   ],
   "source": [
    "# import the model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize the regression models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(),\n",
    "    'AdaBoost': AdaBoostRegressor(),\n",
    "    'Support Vector Regression': SVR(),\n",
    "    'K-Nearest Neighbors': KNeighborsRegressor(),\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'Neural Network': MLPRegressor(max_iter=1000)  # Increase max_iter for larger datasets\n",
    "}\n",
    "\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    results.append((name, rmse))\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'RMSE'])\n",
    "\n",
    "# Sort by RMSE in ascending order\n",
    "results_df = results_df.sort_values(by='RMSE', ascending=True)\n",
    "\n",
    "# Print the best model and its RMSE\n",
    "print(\"Best Model:\")\n",
    "print(results_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error for Gradient Boosting Regressor = 492538.12\n"
     ]
    }
   ],
   "source": [
    "gbr2 = GradientBoostingRegressor()\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150], # Number of boosting stages to be run\n",
    "    'learning_rate': [0.05, 0.1, 0.2], # Shrinkage parameter to prevent overfitting\n",
    "    'max_depth': [3, 4, 5], # Maximum depth of the individual trees\n",
    "}\n",
    "\n",
    "# Perform KFold cross-validation for hyperparameter tuning\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_params = None\n",
    "best_rmse = float('inf')\n",
    "\n",
    "for n_estimators in param_grid['n_estimators']:\n",
    "    for learning_rate in param_grid['learning_rate']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            rmses = []\n",
    "            for train_idx, val_idx in kf.split(X):\n",
    "                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "                gbr2 = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth)\n",
    "                gbr2.fit(X_train, y_train)\n",
    "                y_pred = gbr2.predict(X_val)\n",
    "                rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "                rmses.append(rmse)\n",
    "\n",
    "            mean_rmse = np.mean(rmses)\n",
    "            if mean_rmse < best_rmse:\n",
    "                best_rmse = mean_rmse\n",
    "                best_params = {\n",
    "                    'n_estimators': n_estimators,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'max_depth': max_depth\n",
    "                }\n",
    "\n",
    "# Initialize the Gradient Boosting Regressor with the best hyperparameters\n",
    "best_gbr2 = GradientBoostingRegressor(**best_params)\n",
    "\n",
    "# Train the model on the training data\n",
    "best_gbr2.fit(X, y)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_gbr2.predict(X_test)\n",
    "\n",
    "# Calculate and print RMSE\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"Root Mean Squared Error for Gradient Boosting Regressor = {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
